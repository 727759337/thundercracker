====================
SVM Validator Design
====================

M. Elizabeth Scott, 2012
(C) Sifteo, Inc.

Scope
-----

This describes a few of the different approaches I've considered for
implementing a validator. The validator's job is to take in a block of code,
and emit a single number which indicates how many instructions, starting from
the beginning of the buffer, are safe to use as jump targets.

Base case
---------

In general, the validator is a fairly complex graph theory problem: Each
instruction is a node in a DAG, with edges pointing to any instruction it can
jump to. Instructions can be segmented into three different categories:

  - Definitely valid. These instructions are _terminators_, i.e. they do not
    transfer control back to the same block of code, only to things outside
    the block. This category includes syscalls or svcs which do not return,
    such as return-from-function, long jumps, and syscalls which don't return
    like _SYS_abort().

  - Definitely invalid. This includes any instruction which is outside our
    supported subset of Thumb-2, or any instruction which has a successor
    that lies outside the block: For example an arithmetic instruction at
    the very end of the block, or a jump with a target outside the block.

  - Maybe valid. These are instructions that are, themselves, valid instructions.
    But they can have successors of unknown validity.

So, one formulation of this problem would be to first classify all instructions
into one of these three buckets, then iteratively try to move instructions
from 'maybe' to one of the other two buckets. If this algorithm were performed
in reverse order (last instruction to first), it should converge in only a couple
iterations in most cases.

Simplifying the algorithm
-------------------------

The above algorithm could be implemented pretty efficiently in a bit-parallel
way, using one bit per instruction (or per 32-bit instruction bundle), and doing
most of the propagation by creating masks of successors and operating on 32
bundles at a time with bitwise math.

But, it's overkill. SVM gives us a very strict restriction to operate under:
All non-instruction words must be at the end of a block. This means if we *ever*
find an instruction which is Definitely Invalid, that means the entire rest of the
block is also Definitely Invalid. We need this restriction in order to reduce
the amount of storage space necessary for the validation results. With SVM's
single split point between code and data, we need only store one 8-bit value
per cached code page, instead of one bit per instruction.

Using this property, we can imagine two additional algorithms:

  - Store the split point explicitly, and validate it at load-time. This is the
    simplest to implement at runtime, but it comes at a space penalty of at
    least one byte per page (1/256th of the program). Worse than that, though,
    is the collateral damage caused by having this immovable object at the
    end of each page. More complexity for the compiler, and less flexibility in
    the future.

  - Iterative solution based on making a guess at the split point using only
    Definitely Invalid instuctions, then iteratively refining if necessary.
    This is the algorithm we'll explore next.

Iterative refinement
--------------------

The goal of this algorithm is to take advantage of the fact that SVM validation
is *usually* a really really easy problem, even though the generalized problem
is somewhat hard.

SVM's block partitioning guarantee means that, instead of solving a heavy duty
graph theory problem in which there may be an arbitrary number of valid sub-graphs
of code within the same block, we're only looking for a single DAG. And that DAG
is guaranteed to be 'closed' in the set of instructions resulting from partitioning
the block into exactly two pieces. Instructions 0 through N-1 form a closed DAG,
and instructions N and on are assumed invalid.

This means that instead of tracking the whole DAG, we can think of the problem as
simply estimating and refining a value for 'N'.

In fact, this isn't so different from what we'd be doing if we were to store the
split point explicitly. Storing N lets us take advantage of the fact that checking
whether N might be valid is cheaper than finding the highest valid N.

But really, there is a limited number of possible values for N, and in the common
case our fast ugly first-approximation guess will actually be the right value.

So, let's think about this problem as an iterative refinement of a guess. At first
we scan through the block looking for the first Definitely Invalid instruction.
At this point we store an initial value for N, and we have no need to look at the
rest of the block.

Now we can go back and look at the "maybe" instructions. Using the current guess
of N, are these instructions all valid? If we find an instruction that is invalid
with the current value of N, we know that N can never be higher than this guess.
Our guess is an upper bound. So, no further refinement to N can ever make an invalid
instruction valid, only the reverse. We can then use this knowledge to mark this
"maybe" instruction as "definitely invalid", and revise our upper bound of N to
include only instructions prior to this point.

This algorithm iterates. If we ever make it through the whole block without refining
N, we're done- every instruction is valid with the current guess at N. If we do
decrease N, we're guaranteed to make forward progress through the algorithm. Eventually
N would reach zero, and validation would end.

Optimizations
-------------

There are a few ways we can optimize this to avoid having to do repeat work. Any
instructions which are "definitely valid" (for *any* value of N, not just the current
value) never need to be examined again. We can use a bit vector to keep track of
which instructions need re-examination on the next iteration through the algorithm.

We can also keep track of N as both an upper and a lower bound. The lower bound would
be set based on how many "definitely valid" instructions we have. If a "maybe" instruction
has a successor which is "definitely valid", we can mark that "maybe" instruction as
"definitely valid" and include it below this lower bound. This also saves us from
re-examining instructions we've already looked at.

Bundle Decoding
---------------

The primary building block we use to implement this algorithm involves "decoding"
one instruction bundle to determine (a) if it's valid at all, and (b) what other
instructions could be its successors.

SVM requires all branch targets to be 32-bit aligned, so that for the purposes of
code validation we can always treat the program as an aligned array of 32-bit
"bundles" which contain either one Thumb-2 instruction or two 16-bit Thumb
instructions.

The maximum number of these successor addresses in a bundle is three:
one for the instruction following the bundle, if it's reachable, and
one for a possible jump target for each of the two instructions.

For the validation algorithm this successor information can be distilled
to a single integer value: the "maximum successor" of that bundle. If the bundle
has no successors, this is -1. If the bundle is completely invalid, this returns
an arbitrary large number which is never a valid successor address.

Single pass iteration
---------------------

This iterative algorithm keeps track of two kinds of state

  - An upper bound for the result, which is iteratively refined. This
    starts out as the full size of the block, and we decrease it any
    time we find an instruction bundle which wouldn't be valid.

  - The maximum successor out of all bundles up to the iteration's
    upper bound. If this is >= the upper bound, it indicates that
    the algorithm has not yet converged and the DAG is not closed.
    If it is < the upper bound, we are done and all bundles in the
    range are valid.

How do we know this algorithm will always converge? At each iteration,
we need to know that the upper bound will be decreased. If the algorithm
has continued to iterate, that guarantees that there is at least one
bundle which now has a successor >= the upper bound. On the next iteration,
that bundle will be known to be invalid, causing it to now be excluded
from the set of possible results by decreasing the upper bound. Therefore,
all iterations of this algorithm are guaranteed to make progress by
decreasing the upper bound.

An upper bound of zero always indicates we're finished. This means that
zero bundles have been examined, and the maximum successor will take on
its base-case value of -1.

There are a few specific decisions which help this algorithm converge
in significantly fewer passes than otherwise:

  - We do NOT update our 'maximum successor' estimate for the instruction
    which ends up updating upperBound. This lets us complete in one fewer pass
    than if we always updated the maximum successor. This is safe because,
    at the moment we're processing that bundle, we know it will not be
    included in the set of valid results.

  - Situations where many valid-looking instructions trail after the
    last terminator can be a problem for performance. If we iterated
    forward, we would need one full iteration to propagate each "i+1"
    successor backwards. It would be bubble-sort, effectively. To
    combat this, we can reverse our iteration order.

 With these tweaks, this algorithm is actually _guaranteed_ to run in one
 pass. We can at all times guarantee that our "maximum successor" is less
 than the upper bound.
 
